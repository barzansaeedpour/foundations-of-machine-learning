{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to RNNs\n",
    "    1.1. What is Sequential Data?\n",
    "    Sequential data is any data where the order matters. Examples include:\n",
    "\n",
    "    Time Series Data: Stock prices, weather data, etc.\n",
    "    Text Data: Sentences, where each word's meaning can depend on the previous ones.\n",
    "    Audio Data: Speech signals, where sounds are sequenced over time.\n",
    "    \n",
    "    1.2. Limitations of Traditional Neural Networks\n",
    "    Traditional feedforward neural networks (like Multi-Layer Perceptrons) process inputs independently and cannot capture the dependencies between     different inputs in a sequence. This makes them unsuitable for tasks where context or memory is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional feedforward neural networks (FFNNs) are the most basic type of artificial neural networks. They are called \"feedforward\" because the information in these networks moves in one directionâ€”from the input layer, through the hidden layers (if any), to the output layer. There are no cycles or loops in the network, distinguishing them from recurrent neural networks (RNNs). Let's delve into the details of FFNNs, starting from the basics and moving toward more complex concepts.\n",
    "\n",
    "1. Basic Structure of Feedforward Neural Networks\n",
    "    1.1. Neurons and Layers\n",
    "    Neuron: The fundamental unit of a neural network. Each neuron receives input, processes it (using a weighted sum and an activation function), and passes the result to the next layer.\n",
    "\n",
    "    Layers:\n",
    "\n",
    "    Input Layer: The first layer in the network, which receives the input data. The number of neurons in this layer equals the number of features in the input data.\n",
    "    Hidden Layers: Intermediate layers between the input and output layers. These layers perform computations on the input data and extract relevant features. A network can have one or multiple hidden layers.\n",
    "    Output Layer: The final layer, which produces the output. The number of neurons in this layer depends on the task (e.g., for binary classification, there would typically be one output neuron).\n",
    "\n",
    "    1.2. Forward Pass\n",
    "    In an FFNN, data moves in one direction: forward through the network. During the forward pass:\n",
    "\n",
    "    Each neuron in the hidden layers computes a weighted sum of its inputs.\n",
    "    The weighted sum is passed through an activation function to produce the neuron's output.\n",
    "    The output from one layer serves as the input to the next layer.\n",
    "    Finally, the output layer produces the final predictions.\n",
    "\n",
    "2. Mathematical Formulation\n",
    "    2.1. Weight and Bias\n",
    "    Weights (ð‘Š): Each connection between neurons in adjacent layers has an associated weight. These weights determine the strength and direction (positive or negative) of the influence that one neuron's output has on another neuron's input.\n",
    "    Bias (ð‘): A bias term is added to the weighted sum of inputs to allow the activation function to shift left or right. This provides the model with additional flexibility.\n",
    "    2.2. Activation Functions\n",
    "    The output of each neuron is passed through an activation function, which introduces non-linearity into the model. Common activation functions include:\n",
    "\n",
    "Sigmoid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basic Structure of RNNs\n",
    "    2.1. RNN Architecture\n",
    "    An RNN is structured to maintain a hidden state that captures information about previous elements in the sequence. The key components are:\n",
    "\n",
    "    Input Layer: Takes the sequential input.\n",
    "    Hidden Layer: Processes the input and updates the hidden state.\n",
    "    Output Layer: Produces the output for the current time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step ð‘¡:\n",
    "\n",
    "ð‘¥ð‘¡ â€‹is the input at time step ð‘¡.\n",
    "â„Žð‘¡ is the hidden state at time step ð‘¡.\n",
    "ð‘¦ð‘¡ is the output at time step ð‘¡.\n",
    "\n",
    "The recurrence relation is given by:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ðœŽ is an activation function (commonly tanh or ReLU), ð‘Šâ„Žð‘¥,ð‘Šâ„Žâ„Ž,ð‘Šâ„Žð‘¦â€‹ are weight matrices, and ð‘â„Ž,ð‘ð‘¦ are biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training RNNs\n",
    "    3.1. Backpropagation Through Time (BPTT)\n",
    "    Training RNNs involves adjusting the weights based on errors calculated at each time step. However, unlike feedforward networks, RNNs require   Backpropagation Through Time (BPTT), where the gradients are calculated by unfolding the network across time steps. This process considers how each  input influences not only the current output but also future outputs.\n",
    "\n",
    "    3.2. Challenges in Training RNNs\n",
    "    Vanishing Gradient Problem: The gradients can shrink exponentially as they are propagated backward, making it difficult to train on long sequences.\n",
    "    Exploding Gradient Problem: The gradients can grow exponentially, leading to unstable training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) Repository\n",
    "\n",
    "This repository serves as a comprehensive guide to Natural Language Processing (NLP), covering various concepts, techniques, and implementations. Each section includes theoretical explanations and practical code examples.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1.Introduction to NLP\n",
    "2.Text Preprocessing\n",
    "3.Feature Extraction\n",
    "4.Text Classification\n",
    "5.Named Entity Recognition (NER)\n",
    "6.Sentiment Analysis\n",
    "7.Topic Modeling\n",
    "8.Word Embeddings\n",
    "9.Language Models\n",
    "10.Machine Translation\n",
    "11.Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Natural Language Processing (NLP)\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way.\n",
    "\n",
    "Key Concepts\n",
    "\n",
    "1.Tokenization: The process of breaking down text into individual words or subwords.\n",
    "2.Part-of-Speech (POS) Tagging: Assigning grammatical categories (e.g., noun, verb, adjective) to each word in a text.\n",
    "3.Named Entity Recognition (NER): Identifying and classifying named entities (e.g., person names, organizations, locations) in text.\n",
    "4.Syntax and Parsing: Analyzing the grammatical structure of sentences.\n",
    "5.Semantics: Understanding the meaning of words, phrases, and sentences.\n",
    "6.Pragmatics: Interpreting language in context.\n",
    "\n",
    "Applications of NLP\n",
    "\n",
    ".Machine Translation\n",
    ".Sentiment Analysis\n",
    ".Text Summarization\n",
    ".Question Answering Systems\n",
    ".Chatbots and Virtual Assistants\n",
    ".Information Retrieval\n",
    ".Text Classification\n",
    "\n",
    "Basic NLP Pipeline\n",
    "A typical NLP pipeline consists of the following steps:\n",
    "\n",
    "1.Text acquisition\n",
    "2.Text cleaning and preprocessing\n",
    "3.Tokenization\n",
    "4.Feature extraction\n",
    "5.Model training and evaluation\n",
    "6.Prediction or inference\n",
    "\n",
    "In the following sections, we'll explore each of these steps in detail and implement them using popular NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started with NLTK\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. Let's start with a simple example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.']\n",
      "POS Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"Natural language processing is a subfield of artificial intelligence.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Part-of-Speech Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates basic tokenization and part-of-speech tagging using NLTK. In the upcoming sections, we'll dive deeper into these concepts and explore more advanced NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing in NLP\n",
    "\n",
    "Text preprocessing is a crucial step in NLP that involves cleaning and transforming raw text data into a format suitable for analysis. This process helps to reduce noise in the text and improve the performance of NLP models.\n",
    "Common Preprocessing Steps\n",
    "\n",
    "1.Lowercasing: Converting all text to lowercase to ensure consistency.\n",
    "2.Removing punctuation: Eliminating punctuation marks that may not contribute to the meaning.\n",
    "3.Removing numbers: Removing numerical digits if they're not relevant to the analysis.\n",
    "4.Removing whitespace: Stripping extra spaces, tabs, and newlines.\n",
    "5.Removing stop words: Eliminating common words that don't carry much meaning (e.g., \"the\", \"is\", \"at\").\n",
    "6.Stemming: Reducing words to their root form (e.g., \"running\" to \"run\").\n",
    "7.Lemmatization: Similar to stemming, but ensures the root word is a valid word (e.g., \"better\" to \"good\").\n",
    "8.Handling contractions: Expanding contractions to their full form (e.g., \"don't\" to \"do not\").\n",
    "9.Removing HTML tags: Cleaning text scraped from websites.\n",
    "10.Handling emojis and special characters: Deciding whether to remove, replace, or keep these elements.\n",
    "\n",
    "Preprocessing with NLTK and spaCy\n",
    "We'll demonstrate text preprocessing using both NLTK and spaCy, two popular NLP libraries in Python.\n",
    "NLTK Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\didgostar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox jump lazi dog 've day\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
    "preprocessed_text = preprocess_text_nltk(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text_spacy\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_digit]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
    "preprocessed_text = preprocess_text_spacy(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll explore how to use these preprocessed texts for various NLP tasks such as feature extraction and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction in NLP\n",
    "Feature extraction is the process of transforming raw text data into numerical features that can be used by machine learning algorithms. This step is crucial in NLP as it bridges the gap between human-readable text and machine-understandable input.\n",
    "Common Feature Extraction Techniques\n",
    "\n",
    "1.Bag of Words (BoW): Represents text as a multiset of words, disregarding grammar and word order.\n",
    "2.Term Frequency-Inverse Document Frequency (TF-IDF): Reflects the importance of a word in a document within a collection.\n",
    "3.Word Embeddings: Dense vector representations of words that capture semantic meanings.\n",
    "4.N-grams: Contiguous sequences of n items from a given text.\n",
    "5.Part-of-Speech (POS) Features: Grammatical features based on the role of words in sentences.\n",
    "6.Named Entity Recognition (NER) Features: Features based on identified named entities in the text.\n",
    "7.Syntactic Features: Based on the syntactic structure of sentences (e.g., dependency parsing).\n",
    "\n",
    "Implementing Feature Extraction\n",
    "We'll demonstrate how to implement Bag of Words, TF-IDF, and Word Embeddings using popular Python libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample texts\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the texts\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the BoW representation\n",
    "print(\"Bag of Words representation:\")\n",
    "print(X.toarray())\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample texts (same as before)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the texts\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "print(X.toarray())\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings with Gensim (Word2Vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample texts (same as before)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps all day.\",\n",
    "    \"The quick brown fox is quick.\"\n",
    "]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_corpus = [word_tokenize(text.lower()) for text in corpus]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "print(\"Vector for 'fox':\", model.wv['fox'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"Words similar to 'quick':\", model.wv.most_similar('quick'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples demonstrate how to extract features from text data using different techniques. In the next sections, we'll explore how to use these features for various NLP tasks such as text classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
