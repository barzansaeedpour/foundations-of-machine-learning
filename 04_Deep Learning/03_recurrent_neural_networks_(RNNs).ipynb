{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbpp4R6IeviG"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/nlp.jfif\" width=\"100%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6VMai8eviL"
      },
      "source": [
        "Natural Language Processing (NLP) is a field at the intersection of artificial intelligence, linguistics, and computer science. It focuses on enabling machines to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP encompasses a wide range of tasks, including language translation, sentiment analysis, text summarization, and more.\n",
        "\n",
        "## What is NLP?\n",
        "\n",
        "\n",
        "\n",
        "###  Overview\n",
        "\n",
        "NLP involves the interaction between computers and human languages, enabling machines to process and analyze large amounts of natural language data. This involves tasks like:\n",
        "\n",
        "- **Text Classification**: Assigning categories to text (e.g., spam detection).\n",
        "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
        "- **Part-of-Speech Tagging**: Assigning parts of speech (noun, verb, etc.) to each word in a sentence.\n",
        "- **Sentiment Analysis**: Determining the sentiment or emotional tone of a piece of text.\n",
        "- **Machine Translation**: Automatically translating text from one language to another.\n",
        "- **Question Answering**: Building systems that can answer questions posed in natural language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XAXg_GWeviM"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/digital-twins-and-knowledge-graphs-1280x640.png\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBhfVs_oeviN"
      },
      "source": [
        "##  Processing Natural Language with Neural Networks\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/Traditional Feedforward Neural Networks (FFNNs).JPG\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Neural networks have become the cornerstone of modern NLP, significantly improving the performance of NLP tasks. Below is a breakdown of how different types of neural networks, from traditional feedforward neural networks to transformers, are used in NLP.\n",
        "\n",
        "###  Traditional Feedforward Neural Networks (FFNNs)\n",
        "\n",
        "**Basic Concept**: In NLP, a feedforward neural network can be used for simple tasks like text classification. However, FFNNs treat each word or token in isolation, without considering the sequence or context in which the word appears.\n",
        "\n",
        "**Limitations**:\n",
        "\n",
        "- **Lack of Context Awareness**: FFNNs do not maintain context across words or sentences, making them inadequate for tasks where the order of words matters (e.g., sentiment analysis or language modeling).\n",
        "- **Fixed Input Size**: FFNNs generally require a fixed-size input, which is problematic for variable-length text sequences.\n",
        "\n",
        "Despite these limitations, FFNNs can be combined with other techniques, such as n-grams, to capture some local context, but they still fall short in handling long-term dependencies.\n",
        "\n",
        "\n",
        "\n",
        "Traditional feedforward neural networks (FFNNs) are the most basic type of artificial neural networks. They are called \"feedforward\" because the information in these networks moves in one direction‚Äîfrom the input layer, through the hidden layers (if any), to the output layer. There are no cycles or loops in the network, distinguishing them from recurrent neural networks (RNNs). Let's delve into the details of FFNNs, starting from the basics and moving toward more complex concepts.\n",
        "\n",
        "###  Basic Structure of Feedforward Neural Networks\n",
        "\n",
        "####  Neurons and Layers\n",
        "\n",
        "- **Neuron**: The fundamental unit of a neural network. Each neuron receives input, processes it (using a weighted sum and an activation function), and passes the result to the next layer.\n",
        "\n",
        "- **Layers**:\n",
        "\n",
        "  - **Input Layer**: The first layer in the network, which receives the input data. The number of neurons in this layer equals the number of features in the input data.\n",
        "  \n",
        "  - **Hidden Layers**: Intermediate layers between the input and output layers. These layers perform computations on the input data and extract relevant features. A network can have one or multiple hidden layers.\n",
        "  \n",
        "  - **Output Layer**: The final layer, which produces the output. The number of neurons in this layer depends on the task (e.g., for binary classification, there would typically be one output neuron).\n",
        "\n",
        "###  Forward Pass\n",
        "\n",
        "In an FFNN, data moves in one direction: forward through the network. During the forward pass:\n",
        "\n",
        "- Each neuron in the hidden layers computes a weighted sum of its inputs.\n",
        "- The weighted sum is passed through an activation function to produce the neuron's output.\n",
        "- The output from one layer serves as the input to the next layer.\n",
        "- Finally, the output layer produces the final predictions.\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/feedforward-gif.gif\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "###  Mathematical Formulation\n",
        "\n",
        "####  Weight and Bias\n",
        "\n",
        "- **Weights (ùëä)**: Each connection between neurons in adjacent layers has an associated weight. These weights determine the strength and direction (positive or negative) of the influence that one neuron's output has on another neuron's input.\n",
        "- **Bias (ùëè)**: A bias term is added to the weighted sum of inputs to allow the activation function to shift left or right. This provides the model with additional flexibility.\n",
        "\n",
        "####  Activation Functions\n",
        "\n",
        "The output of each neuron is passed through an activation function, which introduces non-linearity into the model. Common activation functions include:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eGJYooOeviO"
      },
      "source": [
        " - **Sigmoid**:\n",
        "\n",
        " In this formula\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        " Maps the input to a value between 0 and 1 as below fig :\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/sigmoid_activation_function.png\" width=\"50%\"/>\n",
        "</div>        \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwgC5AejeviO"
      },
      "source": [
        " - **Tanh**:\n",
        "\n",
        " In this formula $$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "\n",
        "Maps the input to a value between -1 and 1 as below fig :\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/tanh-fig.jfif\" width=\"50%\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmx4qWxeviP"
      },
      "source": [
        " -  **ReLU (Rectified Linear Unit)**:\n",
        "\n",
        " In this formula $$ \\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "\n",
        " Maps all negative values to 0 and all positive values to the same value.Introduces sparsity by setting negative values to zero and allowing positive values to pass unchanged.\n",
        "\n",
        " <div style=\"text-align: center;\">\n",
        "    <img src=\"./files/relu-fig.png\" width=\"50%\" />\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxChnn22eviQ",
        "outputId": "4e91e7c2-37b3-4419-d698-722ff9346ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO9dTHA-eviR",
        "outputId": "2d589fc6-c9b4-4fc0-b2b0-d58c99da0487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 88ms/step - accuracy: 0.7077 - loss: 0.5292 - val_accuracy: 0.8788 - val_loss: 0.2949\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.9789 - loss: 0.0668 - val_accuracy: 0.8418 - val_loss: 0.4234\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 82ms/step - accuracy: 0.9990 - loss: 0.0077 - val_accuracy: 0.8642 - val_loss: 0.4787\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 7.5762e-04 - val_accuracy: 0.8580 - val_loss: 0.5062\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 3.2040e-04 - val_accuracy: 0.8622 - val_loss: 0.5381\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 1.0803e-04 - val_accuracy: 0.8636 - val_loss: 0.5596\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 6.2260e-05 - val_accuracy: 0.8636 - val_loss: 0.5778\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 3.8099e-05 - val_accuracy: 0.8640 - val_loss: 0.5969\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 2.8016e-05 - val_accuracy: 0.8644 - val_loss: 0.6128\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 1.7094e-05 - val_accuracy: 0.8642 - val_loss: 0.6267\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.8645 - loss: 0.6336\n",
            "Test Accuracy: 0.8637\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(Flatten())  # Flatten the 2D input to 1D for feedforward network\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4_Q1GIUeviS"
      },
      "source": [
        "## Explain Implementation of NLP with Traditional Feedforward Neural Networks (FFNNs)\n",
        "\n",
        "### 1. Dataset:\n",
        "\n",
        "We use the IMDb movie reviews dataset, with reviews labeled as positive or negative.\n",
        "\n",
        "### 2. Preprocessing:\n",
        "\n",
        "- **Tokenization and Padding**: The reviews are already tokenized into integers, and we pad them to ensure they have a fixed length of 500 words.\n",
        "\n",
        "### 3. Model Architecture:\n",
        "\n",
        "- **Embedding Layer**: Converts the integer tokens into dense vectors of fixed size (128-dimensional in this case).\n",
        "- **Flatten Layer**: The output from the Embedding layer is a 2D tensor (sequence length x embedding size). The Flatten layer converts this 2D tensor into a 1D tensor, making it suitable for a feedforward network.\n",
        "- **Dense Layers**:\n",
        "  - The first Dense layer has 64 units with a ReLU activation function, which introduces non-linearity.\n",
        "  - The second Dense layer has 1 unit with a sigmoid activation function, which outputs a probability for binary classification (positive or negative sentiment).\n",
        "\n",
        "### 4. Training:\n",
        "\n",
        "- **Binary Crossentropy**: The loss function used for binary classification.\n",
        "- **Adam Optimizer**: Used for optimizing the model parameters.\n",
        "- **Validation Split**: 20% of the training data is used for validation during training.\n",
        "\n",
        "### 5. Evaluation:\n",
        "\n",
        "After training, the model's accuracy is evaluated on the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VMHWlTaeviS"
      },
      "source": [
        "##  Recurrent Neural Networks (RNNs) for NLP\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/RNN.png\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "###  Introduction to RNNs\n",
        "\n",
        "\n",
        "- **Context Awareness**: Unlike FFNNs, RNNs are designed to handle sequential data, making them well-suited for NLP tasks where context matters. RNNs process input sequences one element at a time, maintaining a hidden state that captures information about previous elements in the sequence.\n",
        "\n",
        "\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "At each time step $ \\ t \\ $, the hidden state $ \\ h_t \\ $ is updated based on the current input $ \\ x_t \\ $ and the previous hidden state $ \\ h_{t-1} \\ $:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Ve3OSleviS"
      },
      "source": [
        "$$\n",
        "h_t = \\sigma(W_{hx}x_t + W_{hh}h_{t-1} + b_h)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qyimG4GeviT"
      },
      "source": [
        "  where   ùëä‚Ñéùë•,ùëä‚Ñé‚Ñé  are weight matrices, ùëè‚Ñé  is a bias term, and ùúé is an activation function (typically tanh or ReLU).\n",
        "  \n",
        "  The output ùë¶ùë°  at each time step can be computed as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAduGwPMeviT"
      },
      "source": [
        "$$\n",
        "y_t = \\sigma(W_{hy}h_t + b_y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5ZHdAceviT"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/RNN-gif.gif\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awCUhmH4eviT"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/rnn-sequence-gif.gif\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRVNFQpPeviT"
      },
      "source": [
        "# Let explain more about Recurrent Neural Networks (RNNs) in NLP\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks particularly well-suited for processing sequential data, making them a popular choice for natural language processing (NLP) tasks. Here's a detailed step-by-step explanation of how RNNs process data in the context of NLP:\n",
        "\n",
        "### 1. **Input Representation**\n",
        "- **Tokenization**: The first step in processing text data is to break down the input text into tokens (words, subwords, or characters).\n",
        "  \n",
        "\n",
        "  - ### Example: Tokenization of a Sentence\n",
        "\n",
        "    **Input Sentence:**  \n",
        "        \"The cat sat on the mat.\"\n",
        "\n",
        "    **Tokenization Process:**  \n",
        "    - **Word-Level Tokenization:**  \n",
        "    The sentence is broken down into individual words (tokens):\n",
        "    ```\n",
        "    [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
        "    ```\n",
        "\n",
        "    - **Subword-Level Tokenization (using Byte Pair Encoding or similar):**  \n",
        "     Each word may be broken down into smaller subword units:\n",
        "    ```\n",
        "    [\"Th\", \"e\", \" ca\", \"t\", \" sa\", \"t\", \" on\", \" the\", \" ma\", \"t\", \".\"]\n",
        "    ```\n",
        "\n",
        "    - **Character-Level Tokenization:**  \n",
        "    The sentence is split into individual characters:\n",
        "    ```\n",
        "    [\"T\", \"h\", \"e\", \" \", \"c\", \"a\", \"t\", \" \", \"s\", \"a\", \"t\", \" \", \"o\", \"n\", \" \", \"t\", \"h\", \"e\", \" \", \"m\", \"a\", \"t\", \".\"]\n",
        "    ```\n",
        "\n",
        "    **Explanation:**\n",
        "\n",
        "    - **Word-Level Tokenization**: This breaks the text into words, making it easy to analyze word-level features.\n",
        "    - **Subword-Level Tokenization**: Useful when dealing with rare words or morphological variations, as it breaks words into more frequent subword units.\n",
        "    - **Character-Level Tokenization**: Useful for tasks where the exact form of text is important, such as in languages with rich morphology or in spell-checking systems.\n",
        "\n",
        "    This example shows how tokenization can vary in granularity depending on the specific needs of the NLP task.\n",
        "- **Embedding**: Each token is then converted into a vector representation, often using pre-trained embeddings like Word2Vec, GloVe, or BERT. The embeddings capture semantic meaning and reduce the dimensionality of the input.\n",
        "    - ### Example: Word Embedding\n",
        "\n",
        "        **Context:**  \n",
        "        Imagine you have the following sentence:\n",
        "        ```\n",
        "        \"The cat sat on the mat.\"\n",
        "        ```\n",
        "\n",
        "        **Step 1: Tokenization**  \n",
        "        The sentence is first broken down into tokens (words):\n",
        "        ```\n",
        "        [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
        "        ```\n",
        "\n",
        "        **Step 2: Word Embedding**  \n",
        "        Each token (word) is then converted into a vector representation. Let's say we use a pre-trained embedding like Word2Vec, which converts each word into a vector of numbers.\n",
        "\n",
        "        For simplicity, let's assume the embeddings map each word to a 3-dimensional vector:\n",
        "\n",
        "        - \"The\" ‚Üí [0.2, 0.1, 0.9]\n",
        "        - \"cat\" ‚Üí [0.8, 0.6, 0.4]\n",
        "        - \"sat\" ‚Üí [0.7, 0.5, 0.3]\n",
        "        - \"on\" ‚Üí [0.4, 0.4, 0.2]\n",
        "        - \"the\" ‚Üí [0.2, 0.1, 0.9] (same as \"The\")\n",
        "        - \"mat\" ‚Üí [0.9, 0.8, 0.7]\n",
        "        - \".\" ‚Üí [0.1, 0.2, 0.3]\n",
        "        \n",
        "        **Output:**  \n",
        "        The sentence is now represented as a sequence of vectors:\n",
        "        ```\n",
        "        [\n",
        "        [0.2, 0.1, 0.9],  // \"The\"\n",
        "        [0.8, 0.6, 0.4],  // \"cat\"\n",
        "        [0.7, 0.5, 0.3],  // \"sat\"\n",
        "        [0.4, 0.4, 0.2],  // \"on\"\n",
        "        [0.2, 0.1, 0.9],  // \"the\"\n",
        "        [0.9, 0.8, 0.7],  // \"mat\"\n",
        "        [0.1, 0.2, 0.3]   // \".\"\n",
        "        ]\n",
        "        ```\n",
        "\n",
        "      <div style=\"text-align: center;\">\n",
        "          <img src=\"./files/introduce NLP.gif\" width=\"50%\"/>\n",
        "      </div>\n",
        "\n",
        "\n",
        "        **Explanation:**\n",
        "\n",
        "        - **Semantic Meaning**: The vectors are designed to capture the semantic meaning of words. For example, words with similar meanings will have similar vectors. If you had another sentence like \"The dog sat on the mat,\" the word \"dog\" might have a vector similar to \"cat.\"\n",
        "  \n",
        "        - **Dimensionality Reduction**: Instead of representing words as large sparse vectors (e.g., one-hot encoding where each word is a huge vector of zeros and a single one), embeddings reduce the dimensionality while preserving meaning. This makes it easier for the model to process and understand the text.\n",
        "\n",
        "        This example shows how embedding transforms words into vectors that a machine learning model can work with, while also preserving the meaning of the words.\n",
        "\n",
        "\n",
        "### 2. **Sequential Data Input**\n",
        "- **Sequence Formation**: The input tokens are arranged in a sequence. For example, a sentence \"The cat sat on the mat\" would be represented as a sequence of vectors corresponding to each word.\n",
        "- **Time Steps**: Each token in the sequence is processed at a different time step in the RNN. The RNN processes the sequence one element at a time, maintaining a hidden state that is updated with each time step.\n",
        "\n",
        "### 3. **RNN Cell Operation**\n",
        "- **Hidden State Initialization**: The RNN starts with an initial hidden state, usually initialized to zero or small random values. This hidden state is updated as the network processes each token in the sequence.\n",
        "- **Processing Each Time Step**:\n",
        "    1. **Current Input**: At time step $ \\ t $, the RNN receives the vector representation of the current token $ \\ x_t $.\n",
        "    2. **Previous Hidden State**: The hidden state from the previous time step $ \\  h_{t-1} $ is also fed into the RNN cell.\n",
        "    3. **Hidden State Update**: The RNN cell combines the current input $ \\ x_t $ and the previous hidden state $ \\ h_{t-1} $ to compute the new hidden state $ \\ h_t $. The update is typically performed using a non-linear function like a tanh or ReLU activation.\n",
        "       $$ h_t = \\tanh\\left(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h\\right) $$\n",
        "\n",
        "       where $ \\ W_{hh} $ and $ \\ W_{xh} $ are weight matrices, and $ \\ b_h $ is a bias term.\n",
        "    4. **Output Generation**: Depending on the task, the RNN might also produce an output $ \\ y_t $ at each time step, which is a function of the current hidden state:\n",
        "       $$\n",
        "       y_t = \\text{softmax}(W_{hy} \\cdot h_t + b_y)\n",
        "       $$\n",
        "       where $ \\ W_{hy} $ is the weight matrix for the output layer, and $ \\ b_y $ is a bias term.\n",
        "\n",
        "### 4. **Handling Long Sequences**\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/rnn.gif\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "- **Vanishing/Exploding Gradient Problem**: When processing long sequences, RNNs can suffer from vanishing or exploding gradients during backpropagation. This means that the network either stops learning (gradients become too small) or becomes unstable (gradients become too large).\n",
        "- **Solutions**:\n",
        "    - **Long Short-Term Memory (LSTM)** and **Gated Recurrent Unit (GRU)** networks are specialized types of RNNs that mitigate these issues by incorporating gates that control the flow of information.\n",
        "    - **Gradient Clipping**: A technique where gradients are capped at a maximum value during training to prevent them from exploding.\n",
        "\n",
        "- ### Explain more about Handling Long Sequences in RNNs\n",
        "\n",
        "    When working with Recurrent Neural Networks (RNNs), handling long sequences of data can be challenging due to the **vanishing** and **exploding gradient problem**. Here‚Äôs a simple explanation of these problems and their solutions:\n",
        "\n",
        "    #### 1. **Vanishing/Exploding Gradient Problem**\n",
        "\n",
        "    - **Vanishing Gradients**:\n",
        "      - **What Happens**: When training an RNN on long sequences, the gradients (which are used to update the model‚Äôs weights) can become very small as they are propagated back through time. As a result, the model stops learning because the updates to the weights become insignificant.\n",
        "      - **Example**: Imagine trying to remember the first word in a long sentence after reading the entire sentence. As you go further into the sentence, your memory of the first word fades away. Similarly, in RNNs, the influence of earlier time steps diminishes as the sequence length increases, making it difficult for the network to learn long-term dependencies.\n",
        "\n",
        "    - **Exploding Gradients**:\n",
        "      - **What Happens**: Conversely, the gradients can become excessively large during backpropagation, causing the model‚Äôs weights to change drastically. This leads to an unstable model that may fail to converge during training.\n",
        "      - **Example**: Think of trying to adjust a radio volume knob. If the knob is too sensitive, a slight touch could make the volume too loud, making the control very difficult. Similarly, in RNNs, large gradients can cause massive weight updates, making the model behave unpredictably.\n",
        "\n",
        "    #### 2. **Solutions to These Problems**\n",
        "\n",
        "    - **Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)**:\n",
        "      - **What They Do**: LSTMs and GRUs are special types of RNNs designed to address the vanishing gradient problem. They include mechanisms called \"gates\" that control the flow of information, allowing the model to keep or forget information as needed over long sequences.\n",
        "      - **Example**: Think of an LSTM as having a memory cell with a gate that controls what information should be remembered and what should be forgotten. This allows the network to remember important information from earlier in the sequence, even if the sequence is long.\n",
        "\n",
        "    - **Gradient Clipping**:\n",
        "      - **What It Does**: Gradient clipping is a technique used to prevent exploding gradients. It sets a maximum threshold for the gradients during backpropagation. If the gradients exceed this threshold, they are scaled down to stay within the limit.\n",
        "      - **Example**: Imagine a speed limit on a highway. If a car tries to go faster than the speed limit, it is forced to slow down to avoid accidents. Similarly, gradient clipping limits the speed (magnitude) of the gradient updates to keep the training process stable.\n",
        "\n",
        "    ### Summary\n",
        "\n",
        "    Handling long sequences in RNNs can be tricky due to the vanishing and exploding gradient problems. LSTMs and GRUs solve the vanishing gradient issue by using gates to manage information flow, while gradient clipping prevents exploding gradients by capping their magnitude. These solutions allow RNNs to learn effectively from long sequences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Backpropagation Through Time (BPTT)**\n",
        "- **Unrolling the RNN**: To compute the gradients for training, the RNN is \"unrolled\" across time steps. This means the sequence of computations is treated as a feedforward network where each time step corresponds to a layer.\n",
        "- **Loss Computation**: The loss is computed for each output at each time step, depending on the task. For example, in language modeling, the loss might be the cross-entropy loss between the predicted word and the actual next word in the sequence.\n",
        "- **Gradient Calculation**: Gradients are calculated for each weight matrix by backpropagating the error through the unrolled network, a process called Backpropagation Through Time (BPTT).\n",
        "- **Parameter Update**: Using the calculated gradients, the model's parameters (weights and biases) are updated using an optimization algorithm like Stochastic Gradient Descent (SGD) or Adam.\n",
        "    - ### Understanding Backpropagation Through Time (BPTT) in RNNs\n",
        "\n",
        "    Backpropagation Through Time (BPTT) is the process used to train Recurrent Neural Networks (RNNs). Here's a simple explanation of how BPTT works, step by step, with an example for clarity:\n",
        "\n",
        "    #### 1. **Unrolling the RNN**\n",
        "\n",
        "    - **What It Means**:\n",
        "      - RNNs process sequences one step at a time, reusing the same weights at each step. To compute gradients for training, the RNN is \"unrolled\" across time steps, meaning we treat each time step as a separate layer in a deep network.\n",
        "      - **Example**: Imagine you have a sentence, \"The cat sat,\" and the RNN processes one word at a time. Unrolling the RNN would look like a feedforward neural network with three layers, each corresponding to one word in the sentence (\"The,\" \"cat,\" and \"sat\").\n",
        "\n",
        "        $$ \\texttt{Layer 1: \"The\"‚ÜíLayer 2: \"cat\"‚ÜíLayer 3: \"sat\"} $$\n",
        "\n",
        "\n",
        "    #### 2. **Loss Computation**\n",
        "\n",
        "    - **What It Means**:\n",
        "      - The RNN makes predictions at each time step (e.g., predicting the next word in a sequence). The loss is calculated based on how far off the predictions are from the actual values.\n",
        "      - **Example**: If the task is to predict the next word in the sentence, the RNN might predict \"dog\" instead of \"cat\" after \"The\". The loss function (like cross-entropy loss) measures the difference between the predicted word (\"dog\") and the actual word (\"cat\").\n",
        "\n",
        "    #### 3. **Gradient Calculation**\n",
        "\n",
        "    - **What It Means**:\n",
        "      - Gradients represent how much the model's weights need to change to reduce the loss. BPTT calculates these gradients by backpropagating the error through the unrolled network.\n",
        "      - **Example**: After unrolling, we go backward from \"sat\" to \"The,\" calculating the gradients for each layer (time step). The error at each time step affects not only the weights at that step but also the weights at previous steps.\n",
        "\n",
        "    #### 4. **Parameter Update**\n",
        "\n",
        "    - **What It Means**:\n",
        "      - Once the gradients are calculated, they are used to update the model‚Äôs weights to minimize the loss. This is done using optimization algorithms like Stochastic Gradient Descent (SGD) or Adam.\n",
        "      - **Example**: After computing the gradients, the model updates its weights so that next time it encounters a similar sequence, it might predict \"cat\" instead of \"dog\" after \"The\".\n",
        "\n",
        "    ### Summary with a Simple Example\n",
        "\n",
        "    Imagine teaching a child to complete the sentence \"The cat sat on the ...\". Initially, the child might guess wrong (\"dog\" instead of \"mat\"). You correct them, and they adjust their understanding.\n",
        "\n",
        "    In RNN training:\n",
        "    1. **Unrolling**: The sentence is split into steps (\"The,\" \"cat,\" \"sat,\" \"on,\" \"the,\" \"mat\").\n",
        "    2. **Loss Computation**: The model guesses the next word at each step. It compares its guesses with the actual words and calculates the loss.\n",
        "    3. **Gradient Calculation**: The model backtracks through the sentence, figuring out how to adjust its guesses to be more accurate.\n",
        "    4. **Parameter Update**: It tweaks its \"understanding\" (weights) to do better next time.\n",
        "\n",
        "    BPTT is like teaching the model by repeatedly correcting its mistakes until it gets better at making predictions for sequences of words.\n",
        "\n",
        "### 6. **Task-Specific Adjustments**\n",
        "- **Sequence-to-Sequence Tasks**: In tasks like machine translation, the RNN is often used in an encoder-decoder architecture. The encoder processes the input sequence, and the decoder generates the output sequence.\n",
        "- **Attention Mechanism**: In more advanced models, attention mechanisms are incorporated to allow the RNN to focus on specific parts of the input sequence when generating each output.\n",
        "\n",
        "### 7. **Inference**\n",
        "- **Sequence Generation**: After training, the RNN can generate sequences of text. In a sequence generation task, the model might generate the next word in a sentence by sampling from the predicted probability distribution over the vocabulary.\n",
        "- **Handling Variable-Length Sequences**: RNNs are capable of processing input sequences of variable length, making them flexible for a wide range of NLP tasks.\n",
        "\n",
        "### 8. **Evaluation**\n",
        "- **Perplexity**: In language modeling, the performance of an RNN is often evaluated using perplexity, which measures how well the model predicts the next word in a sequence.\n",
        "- **Accuracy and F1 Score**: For classification tasks like sentiment analysis, metrics like accuracy, precision, recall, and F1 score are commonly used.\n",
        "\n",
        "### 9. **Common Applications in NLP**\n",
        "- **Language Modeling**: Predicting the next word in a sentence.\n",
        "- **Machine Translation**: Translating text from one language to another.\n",
        "- **Sentiment Analysis**: Classifying the sentiment of a piece of text.\n",
        "- **Speech Recognition**: Converting spoken language into text.\n",
        "- **Named Entity Recognition (NER)**: Identifying and classifying entities in text.\n",
        "\n",
        "This detailed process shows how RNNs handle sequential data in NLP tasks, from the initial input representation to the final output and evaluation. The adaptability of RNNs to different tasks and their ability to process variable-length sequences make them a foundational model in NLP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Backpropagation in Recurrent Neural Networks (RNNs) is a complex process because RNNs involve time-dependent data, which requires calculating gradients not only over the current time step but also over previous time steps. This process is called **Backpropagation Through Time (BPTT)**.\n",
        "\n",
        "Here‚Äôs a step-by-step explanation of the backpropagation process in RNNs, including the key formulae and explanations:\n",
        "\n",
        "### 1. **RNN Overview**\n",
        "An RNN takes a sequence of inputs $ \\ x_1, x_2, ..., x_T $, where each input is processed in a time-dependent manner. At each time step $ \\ t $, the RNN has:\n",
        "- Input vector $ \\ x_t $,\n",
        "- Hidden state vector $ \\ h_t\\ $, and\n",
        "- Output vector $ \\ y_t\\ $.\n",
        "\n",
        "The hidden state at time $ \\ t \\ $ is computed based on the current input and the previous hidden state:\n",
        "$$\n",
        "\\\n",
        "h_t = f(W_h h_{t-1} + W_x x_t)\n",
        "\\\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\ f $ is a nonlinear activation function (e.g.,$ \\ tanh \\ $ , $ \\ ReLU \\ $  ),\n",
        "- $ \\ W_h \\  $ and $ \\ W_x \\  $ are weight matrices for the hidden state and input respectively.\n",
        "\n",
        "The output is typically a function of the hidden state:\n",
        "$$\n",
        "\\\n",
        "y_t = g(W_y h_t)\n",
        "\\\n",
        "$$\n",
        "Where $ \\ g \\  $ is the output activation function and $ \\ W_y \\  $ is the weight matrix for the output.\n",
        "\n",
        "### 2. **Loss Function**\n",
        "A loss function $ \\ L_t \\  $ at each time step is defined to measure how far the predicted output $ \\ y_t\\  $ is from the true label $ \\ y_t^{true}\\  $. For instance, in a classification task:\n",
        "$ \\ \n",
        "L_t = \\text{cross-entropy}(y_t^{true}, y_t)\n",
        "\\ $\n",
        "\n",
        "The total loss across all time steps is the sum of individual time step losses:\n",
        "$ \\ \n",
        "L = \\sum_{t=1}^{T} L_t\n",
        "\\ $\n",
        "\n",
        "### 3. **Backpropagation Through Time (BPTT)**\n",
        "To minimize the total loss $ \\ L\\ $, we use gradient-based optimization techniques like stochastic gradient descent (SGD). For that, we need to compute the gradients of the loss with respect to the RNN's parameters: $ \\ W_h\\ $, $ \\ W_x\\ $, and $ \\ W_y\\ $. This is where BPTT comes into play.\n",
        "\n",
        "#### 3.1 **Gradient Calculation for Output Weights \\(W_y\\)**\n",
        "The gradient of the loss with respect to $ \\ W_y\\ $ is straightforward, as it's only dependent on the current time step $ \\ t\\ $:\n",
        "$$ \\ \n",
        "   frac{\\partial L}{\\partial W_y} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
        "\\ $$\n",
        "Where:\n",
        "- $ \\ \\frac{\\partial L_t}{\\partial y_t}\\ $ is the derivative of the loss with respect to the output at time $ \\ t\\ $,\n",
        "- $ \\ \\frac{\\partial y_t}{\\partial W_y}\\ $ is the derivative of the output with respect to the weight matrix $\\ W_y\\ $.\n",
        "\n",
        "#### 3.2 **Gradient Calculation for Hidden-to-Hidden Weights \\(W_h\\)**\n",
        "The hidden-to-hidden weight \\(W_h\\) has dependencies not only on the current time step but also on previous time steps due to the recurrent structure. The gradient for $ \\ W_h\\ $ can be written as:\n",
        "$ \\ \n",
        "\\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^{T} \\sum_{k=0}^{t} \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_h}\n",
        "\\ $\n",
        "Where:\n",
        "- $  \\ \\frac{\\partial h_t}{\\partial h_k}\\ $ represents the dependency of the current hidden state $ \\ h_t\\ $ on a previous hidden state $ \\ h_k\\ $.\n",
        "\n",
        "#### 3.3 **Gradient Calculation for Input-to-Hidden Weights \\(W_x\\)**\n",
        "Similarly, the input-to-hidden weight $ \\ W_x\\ $ affects the hidden states directly at each time step:\n",
        "$ \\ \n",
        "\\frac{\\partial L}{\\partial W_x} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_x}\n",
        "\\ $\n",
        "\n",
        "### 4. **Exploding and Vanishing Gradient Problem**\n",
        "One major challenge in RNNs is the **exploding** or **vanishing gradient problem**, especially when the sequence length $ \\ T\\ $ becomes large. This occurs because the gradient of the hidden states over time can either grow exponentially large (exploding) or shrink to near zero (vanishing), leading to difficulty in training.\n",
        "\n",
        "The key issue here is with the term $ \\ \\frac{\\partial h_t}{\\partial h_k}\\ $, which often involves products of Jacobians of the activation function $ \\ f\\ $. If these Jacobians have eigenvalues larger than 1 (exploding) or smaller than 1 (vanishing), the gradients tend to either explode or vanish.\n",
        "\n",
        "#### Mitigating Exploding/Vanishing Gradients:\n",
        "- **Gradient Clipping**: For exploding gradients, clip the gradients during backpropagation to avoid excessively large updates.\n",
        "- **Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU)**: These architectures are designed to alleviate the vanishing gradient problem by incorporating gates that control the flow of information.\n",
        "\n",
        "### 5. **Chain Rule for BPTT**\n",
        "The BPTT is essentially applying the **chain rule** of calculus over time to propagate the gradients backward from the final time step to the initial time step.\n",
        "\n",
        "For a hidden state $ \\ h_t\\ $, the gradient at time step $ \\ t\\ $ is influenced by the loss at time step $ \\ t\\ $ as well as all subsequent time steps due to recurrence. Mathematically, the gradient for hidden state $ \\ h_t\\ $ can be expressed as:\n",
        "$$ \\ \n",
        "   frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_t}{\\partial h_t} + \\sum_{k=t+1}^{T} \\frac{\\partial L_k}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial h_t}\n",
        "\\ $$\n",
        "This requires computing gradients for each time step and propagating them backwards.\n",
        "\n",
        "### 6. **Summary of Formulae**\n",
        "1. Hidden state update:\n",
        "   $$ \\\n",
        "         h_t = f(W_h h_{t-1} + W_x x_t)\n",
        "   \\ $$\n",
        "2. Output computation:\n",
        "   $$ \\ \n",
        "   y_t = g(W_y h_t)\n",
        "   \\ $$\n",
        "3. Loss function:\n",
        "   $$ \\ \n",
        "   L = \\sum_{t=1}^{T} L_t\n",
        "   \\ $$\n",
        "4. Gradients for parameters:\n",
        "   - $ \\ W_y\\ $:\n",
        "     $$ \\ \n",
        "         \\frac{\\partial L}{\\partial W_y} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
        "     \\ $$\n",
        "   - $ \\ W_h\\ $:\n",
        "     $$ \\\n",
        "         \\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^{T} \\sum_{k=0}^{t} \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_h}\n",
        "     \\ $$\n",
        "   - $ \\ W_x\\ $:\n",
        "     $$ \\\n",
        "         \\frac{\\partial L}{\\partial W_x} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_x}\n",
        "     \\ $$\n",
        "\n",
        "By iterating through these gradients over time, the weights are updated to minimize the loss function, allowing the RNN to learn from the data sequence.\n",
        "\n",
        "Let me know if you need further clarifications or examples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7eRcbkSeviU"
      },
      "source": [
        "\n",
        "###  Challenges with RNNs\n",
        "\n",
        "- **Vanishing Gradient Problem**: As the length of the input sequence increases, the gradients used to update the network's weights during backpropagation can become very small, making it difficult to learn long-term dependencies.\n",
        "- **Exploding Gradient Problem**: Conversely, the gradients can also become excessively large, leading to unstable training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otnuCK_LeviV",
        "outputId": "ed84481b-bfe8-46cc-c453-0e30790e7812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 215ms/step - accuracy: 0.5640 - loss: 0.6729 - val_accuracy: 0.7102 - val_loss: 0.5579\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 216ms/step - accuracy: 0.7316 - loss: 0.5280 - val_accuracy: 0.6652 - val_loss: 0.6097\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 227ms/step - accuracy: 0.8007 - loss: 0.4363 - val_accuracy: 0.7106 - val_loss: 0.5946\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 218ms/step - accuracy: 0.8361 - loss: 0.3697 - val_accuracy: 0.7030 - val_loss: 0.6409\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 215ms/step - accuracy: 0.8822 - loss: 0.2846 - val_accuracy: 0.6428 - val_loss: 0.6864\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 214ms/step - accuracy: 0.7807 - loss: 0.4498 - val_accuracy: 0.6016 - val_loss: 0.6932\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 216ms/step - accuracy: 0.7622 - loss: 0.4726 - val_accuracy: 0.6192 - val_loss: 0.7538\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 218ms/step - accuracy: 0.8270 - loss: 0.3775 - val_accuracy: 0.6474 - val_loss: 0.7583\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 217ms/step - accuracy: 0.8877 - loss: 0.2811 - val_accuracy: 0.7100 - val_loss: 0.7101\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 222ms/step - accuracy: 0.9246 - loss: 0.2056 - val_accuracy: 0.6970 - val_loss: 0.7955\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7138 - loss: 0.7572\n",
            "Test Accuracy: 0.7136\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(SimpleRNN(128, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gpdiG7VeviV"
      },
      "source": [
        "# Explain Implementation of RNN\n",
        "\n",
        "1. **Dataset:**\n",
        "\n",
        "    **IMDB Dataset**: We use the IMDB movie reviews dataset, which contains 50,000 reviews labeled as positive or negative. We use 25,000 reviews for training and 25,000 for testing.\n",
        "\n",
        "2. **Preprocessing:**\n",
        "\n",
        "    **Tokenization**: The reviews are already tokenized as integers where each integer represents a word in a dictionary.\n",
        "    \n",
        "    **Padding**: Since RNNs require fixed-length input sequences, we pad the sequences to ensure they are all the same length.\n",
        "\n",
        "3. **Model Architecture:**\n",
        "\n",
        "    **Embedding Layer**: This layer turns positive integers (representing words) into dense vectors of fixed size. It helps in capturing semantic meanings of the words.\n",
        "    \n",
        "    **SimpleRNN Layer**: A basic RNN layer that processes the sequence of word embeddings and maintains a hidden state that captures information about the sequence.\n",
        "    \n",
        "    **Dense Layer**: A fully connected layer with a sigmoid activation function to output a probability value for binary classification (positive or negative sentiment).\n",
        "\n",
        "4. **Training:**\n",
        "\n",
        "    **Binary Crossentropy**: Used as the loss function because this is a binary classification problem.\n",
        "    \n",
        "    **Adam Optimizer**: A popular optimizer that adapts the learning rate during training.\n",
        "    \n",
        "    **Validation Split**: We use 20% of the training data for validation during training.\n",
        "\n",
        "5. **Evaluation:**\n",
        "\n",
        "    After training, the model is evaluated on the test dataset to measure its accuracy in predicting sentiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s23O7tEreviV"
      },
      "source": [
        "# Long Short-Term Memory (LSTM) Networks\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/LSTM.png\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "-  **Introduction to LSTMs**\n",
        "\n",
        "    **Motivation**: LSTM networks were designed to address the vanishing gradient problem in RNNs. They introduce a memory cell that can maintain its state over long periods, along with gates that regulate the flow of information into and out of the cell.\n",
        "\n",
        "    <div style=\"text-align: center;\">\n",
        "    <img src=\"./files/1_goJVQs-p9kgLODFNyhl9zA.gif\" width=\"50%\"/>\n",
        "    </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xeLuQJoeviW"
      },
      "source": [
        "### 1. Cell State\n",
        "\n",
        "**Cell State Overview:**\n",
        "- The cell state, often denoted as $ \\ C_t \\ $, is a crucial component of an LSTM unit. It acts as a kind of long-term memory, carrying information throughout the sequence. Unlike the hidden state, which is more focused on the current output, the cell state maintains information over long periods.\n",
        "\n",
        "**How It Works:**\n",
        "- The cell state is essentially a conveyor belt running through the entire LSTM chain. Information can be added to or removed from this conveyor belt through various gates.\n",
        "- It‚Äôs updated by combining the old cell state and new candidate values, allowing the network to retain useful information for long periods.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- Suppose the previous cell state is $ \\ C_{t-1} \\ $ and the new information to be added is  $ \\tilde{C}_t \\ $. The updated cell state $ \\ C_t \\ $ can be computed as:\n",
        "  $$ \\\n",
        "  C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
        "  \\ $$\n",
        "  Here:\n",
        "  - $ \\ f_t \\ $ is the forget gate's output.\n",
        "  - $ \\ i_t \\ $ is the input gate's output.\n",
        "  - $ \\ \\tilde{C}_t \\ $ is the new candidate values for the cell state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWSZJBjmeviW"
      },
      "source": [
        "### 2. Forget Gate\n",
        "\n",
        "**Forget Gate Overview:**\n",
        "- The forget gate, denoted as $ \\ f_t  \\ $, controls which parts of the cell state should be discarded. It essentially decides what old information should be \"forgotten.\"\n",
        "\n",
        "**How It Works:**\n",
        "- The forget gate uses a sigmoid activation function to produce values between 0 and 1. Each element of the cell state is multiplied by this value.\n",
        "- A value of 0 means ‚Äúcompletely forget‚Äù and a value of 1 means ‚Äúcompletely retain.‚Äù\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The forget gate output is calculated as:\n",
        "  $$ \\\n",
        "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "  \\ $$\n",
        "  Where:\n",
        "  - $ \\ \\sigma  \\ $ denotes the sigmoid function.\n",
        "  - $ \\ W_f  \\ $ is the weight matrix for the forget gate.\n",
        "  - $ \\ h_{t-1}  \\ $ is the previous hidden state.\n",
        "  - $ \\ x_t  \\ $ is the current input.\n",
        "  - $ \\ b_f  \\ $ is the bias term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byViCRaVeviW"
      },
      "source": [
        "### 3. Input Gate\n",
        "\n",
        "**Input Gate Overview:**\n",
        "- The input gate, denoted as $ \\ i_t \\ $, decides which new information will be added to the cell state. It controls how much of the new information should be incorporated into the existing cell state.\n",
        "\n",
        "**How It Works:**\n",
        "- The input gate has two parts:\n",
        "  - **Sigmoid Layer**: Determines which values to update.\n",
        "  - **Tanh Layer**: Generates new candidate values to be added to the cell state.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The input gate output is calculated as:\n",
        "  $$ \\\n",
        "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "  \\ $$\n",
        "  Where:\n",
        "  - $ \\ \\sigma \\ $ is the sigmoid function.\n",
        "  - $ \\ W_i \\ $ is the weight matrix for the input gate.\n",
        "  - $ \\ h_{t-1} \\ $ is the previous hidden state.\n",
        "  - $ \\ x_t \\ $ is the current input.\n",
        "  - $ \\ b_i \\ $ is the bias term.\n",
        "\n",
        "- The new candidate values are calculated as:\n",
        "  $$ \\\n",
        "  \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "  \\ $$\n",
        "  Where:\n",
        "  - $ \\ \\tanh \\ $  is the hyperbolic tangent function.\n",
        "  - $ \\ W_C \\ $ is the weight matrix for the candidate values.\n",
        "  - $ \\ b_C \\ $ is the bias term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YE_vqGReviW"
      },
      "source": [
        "### 4. Output Gate\n",
        "\n",
        "**Output Gate Overview:**\n",
        "- The output gate, denoted as $ \\ o_t \\ $, controls what part of the cell state will be output to the next hidden state. It determines the final output of the LSTM unit.\n",
        "\n",
        "**How It Works:**\n",
        "- The output gate uses a sigmoid function to decide which parts of the cell state will contribute to the hidden state.\n",
        "- The cell state is then processed through a tanh function to squish the values between -1 and 1 before being multiplied by the output gate's result.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The output gate is calculated as:\n",
        "  $$ \\\n",
        "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "  $$ \\\n",
        "  Where:\n",
        "  - $ \\ \\sigma \\ $ is the sigmoid function.\n",
        "  - $ \\ W_o \\ $ is the weight matrix for the output gate.\n",
        "  - $ \\ h_{t-1} \\ $ is the previous hidden state.\n",
        "  - $ \\ x_t \\ $ is the current input.\n",
        "  - $ \\ b_o \\ $ is the bias term.\n",
        "\n",
        "- The hidden state $ \\ h_t \\ $ is then computed as:\n",
        "  $$ \\\n",
        "  h_t = o_t \\cdot \\tanh(C_t)\n",
        "  $$ \\\n",
        "  Where:\n",
        "  -  $ \\tanh(C_t) \\ $ squashes the cell state values to be between -1 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sres4sSTeviW"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/lstm2.png\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9hASX09eviW"
      },
      "source": [
        "\n",
        "\n",
        "### Summary\n",
        "\n",
        "- Cell State $ \\ C_t \\ $: Carries long-term memory and is updated through a combination of the forget gate and input gate.\n",
        "- **Forget Gate $ \\ f_t \\ $: Decides what information to discard from the cell state.\n",
        "- Input Gate $ \\ i_t \\ $: Determines which new information to add to the cell state.\n",
        "- Output Gate $ \\ o_t \\ $: Controls what information from the cell state should be output to the hidden state.\n",
        "\n",
        "Together, these gates and states enable LSTMs to learn and remember information over long sequences, making them well-suited for tasks involving sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwuR7RDeviX"
      },
      "source": [
        "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/forget gate-lstm-gif.gif\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzUsd3IeviX"
      },
      "source": [
        "- **Input Gate**: Decides what new information to add to the cell state.\n",
        "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/lstm inpt -gif.gif\" width=\"50%\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7dZAevNeviX"
      },
      "source": [
        "- **Output Gate**: Decides what to output based on the cell state.\n",
        "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWTozXseviX"
      },
      "source": [
        "- The **cell state** ùê∂ùë°  is updated as follows:\n",
        "\n",
        "$$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t $$\n",
        "\n",
        "where ùê∂~ùë°  is the candidate cell state, typically computed using a tanh activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m0n038MeviX"
      },
      "source": [
        "\n",
        "- The **hidden state** ‚Ñéùë°  is updated as:\n",
        "$$ h_t = o_t * \\tanh(C_t) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4hGOXYneviX"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/lstm-gif.gif\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Y7C5IPeviX"
      },
      "source": [
        "# Explanation of Custom LSTM Cell Implementation\n",
        "\n",
        "**LSTM Gates:**\n",
        "\n",
        "- **Forget Gate**: Determines what part of the previous cell state should be forgotten. It's calculated using a sigmoid activation function. The output of the forget gate is multiplied by the previous cell state to \"forget\" unimportant parts.\n",
        "\n",
        "- **Input Gate**: Controls what new information should be added to the cell state. It has two parts:\n",
        "    - **The input_gate**: Decides which parts of the input are important using a sigmoid function.\n",
        "    - **The input_value**: Determines the potential new values to add to the cell state using a tanh function.\n",
        "\n",
        "- **Cell State Update**: The forget gate's output and the input gate's output are combined to update the cell state.\n",
        "\n",
        "- **Output Gate**: Determines the next hidden state (which is also the output) based on the updated cell state and the current input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2aSJSG7eviZ",
        "outputId": "21bf181f-04e3-4bb1-89f1-67d270b49138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 823ms/step - accuracy: 0.6666 - loss: 0.5944 - val_accuracy: 0.6302 - val_loss: 0.6334\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 916ms/step - accuracy: 0.7629 - loss: 0.4982 - val_accuracy: 0.8416 - val_loss: 0.3765\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 850ms/step - accuracy: 0.8546 - loss: 0.3439 - val_accuracy: 0.8446 - val_loss: 0.3706\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 835ms/step - accuracy: 0.9121 - loss: 0.2269 - val_accuracy: 0.8568 - val_loss: 0.3481\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 897ms/step - accuracy: 0.9517 - loss: 0.1405 - val_accuracy: 0.8726 - val_loss: 0.3602\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 833ms/step - accuracy: 0.9729 - loss: 0.0888 - val_accuracy: 0.8696 - val_loss: 0.3826\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 844ms/step - accuracy: 0.9838 - loss: 0.0562 - val_accuracy: 0.8660 - val_loss: 0.4859\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 830ms/step - accuracy: 0.9897 - loss: 0.0405 - val_accuracy: 0.8706 - val_loss: 0.5181\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 799ms/step - accuracy: 0.9913 - loss: 0.0334 - val_accuracy: 0.8722 - val_loss: 0.5624\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 825ms/step - accuracy: 0.9951 - loss: 0.0195 - val_accuracy: 0.8668 - val_loss: 0.5674\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 257ms/step - accuracy: 0.8503 - loss: 0.6267\n",
            "Test Accuracy: 0.8540\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "embedding_size = 128\n",
        "batch_size = 32\n",
        "lstm_units = 128\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Custom LSTM cell implementation\n",
        "class CustomLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(CustomLSTMCell, self).__init__()\n",
        "        self.units = units\n",
        "        self.state_size = [units, units]\n",
        "        self.output_size = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(input_dim + self.units, 4 * self.units),\n",
        "                                      initializer='glorot_uniform',\n",
        "                                      name='kernel')\n",
        "        self.bias = self.add_weight(shape=(4 * self.units,),\n",
        "                                    initializer='zeros',\n",
        "                                    name='bias')\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        h_prev, c_prev = states\n",
        "        z = tf.matmul(tf.concat([inputs, h_prev], axis=1), self.kernel)\n",
        "        z = z + self.bias\n",
        "\n",
        "        i, f, c, o = tf.split(z, 4, axis=1)\n",
        "\n",
        "        i = tf.sigmoid(i)\n",
        "        f = tf.sigmoid(f)\n",
        "        c = f * c_prev + i * tf.tanh(c)\n",
        "        o = tf.sigmoid(o)\n",
        "\n",
        "        h = o * tf.tanh(c)\n",
        "\n",
        "        return h, [h, c]\n",
        "\n",
        "# Build the LSTM model\n",
        "inputs = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embedding_size)(inputs)\n",
        "\n",
        "lstm_layer = tf.keras.layers.RNN(CustomLSTMCell(lstm_units), return_sequences=False)\n",
        "x = lstm_layer(x)\n",
        "\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KvEqmTmeviZ",
        "outputId": "857feb8c-f4f1-46b8-fa3b-00cace0ef476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 892ms/step - accuracy: 0.6977 - loss: 0.5784 - val_accuracy: 0.8284 - val_loss: 0.4083\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 890ms/step - accuracy: 0.8715 - loss: 0.3165 - val_accuracy: 0.8500 - val_loss: 0.3522\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 880ms/step - accuracy: 0.9078 - loss: 0.2372 - val_accuracy: 0.8688 - val_loss: 0.3370\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 863ms/step - accuracy: 0.9365 - loss: 0.1733 - val_accuracy: 0.8516 - val_loss: 0.3834\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 861ms/step - accuracy: 0.9432 - loss: 0.1491 - val_accuracy: 0.8384 - val_loss: 0.5129\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 861ms/step - accuracy: 0.9640 - loss: 0.1031 - val_accuracy: 0.8470 - val_loss: 0.4764\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 897ms/step - accuracy: 0.9811 - loss: 0.0618 - val_accuracy: 0.7606 - val_loss: 0.6351\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 879ms/step - accuracy: 0.9630 - loss: 0.0976 - val_accuracy: 0.8536 - val_loss: 0.5086\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 884ms/step - accuracy: 0.9885 - loss: 0.0366 - val_accuracy: 0.8298 - val_loss: 0.8918\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 255ms/step - accuracy: 0.8384 - loss: 0.7492\n",
            "Test Accuracy: 0.8390\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(LSTM(128))  # LSTM layer with 128 units\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es_6Q0NYeviZ"
      },
      "source": [
        "# LSTM Applications in NLP\n",
        "\n",
        "- **Machine Translation**: Translating text from one language to another by capturing long-term dependencies between words.\n",
        "\n",
        "- **Text Summarization**: Summarizing a long piece of text by understanding the overall context.\n",
        "\n",
        "- **Speech Recognition**: Converting spoken language into text by processing sequences of audio frames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hd-ESsMeviZ"
      },
      "source": [
        "\n",
        "# Overview of GRU\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/gru3.png\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "The GRU model, introduced in 2014 by Cho et al., simplifies the LSTM architecture by combining the forget and input gates into a single update gate, which reduces the number of parameters and computational complexity. Despite this simplification, GRUs have proven to be effective for many tasks.\n",
        "The Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture that, like Long Short-Term Memory (LSTM) networks, is designed to handle sequential data and capture long-term dependencies. GRUs are often used in natural language processing (NLP) tasks, and they offer a simpler alternative to LSTMs while still addressing some of the key issues faced by traditional RNNs.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/gru4.png\" width=\"50%\"/>\n",
        "</div>\n",
        "\n",
        "### How GRU Works\n",
        "\n",
        "1. **Updating Information**:\n",
        "   - The update gate $ \\ z_t \\ $ decides how much of the previous hidden state $ \\ h_{t-1} \\ $ should be kept and how much new information $ \\tilde{h}_t \\ $ should be added.\n",
        "   - If $ \\ z_t \\ $ is close to 1, the GRU retains most of the previous state. If $ \\ z_t \\ $ is close to 0, the GRU updates the hidden state with the new candidate values.\n",
        "\n",
        "2. **Resetting Information**:\n",
        "   - The reset gate $ \\ r_t \\ $ determines how much of the previous hidden state $ \\ h_{t-1} \\ $ should be forgotten when computing the new candidate activation  $ \\tilde{h}_t \\ $.\n",
        "   - A value close to 0 for $ \\ r_t \\ $ means that the previous state is less influential in computing $ \\tilde{h}_t \\ $, allowing the GRU to focus more on the new input.\n",
        "\n",
        "### Advantages of GRU\n",
        "\n",
        "1. **Simplicity**:\n",
        "   - GRUs have fewer parameters compared to LSTMs because they merge the forget and input gates into a single update gate, making them computationally more efficient and easier to train.\n",
        "\n",
        "2. **Performance**:\n",
        "   - Despite their simplicity, GRUs can perform as well as or better than LSTMs on some tasks, especially when dealing with smaller datasets or simpler architectures.\n",
        "\n",
        "3. **Effective for Sequential Data**:\n",
        "   - Like LSTMs, GRUs are effective for capturing dependencies in sequential data, making them suitable for various NLP tasks.\n",
        "\n",
        "### Applications in NLP\n",
        "\n",
        "GRUs are used in various NLP tasks due to their efficiency and effectiveness. Some applications include:\n",
        "\n",
        "1. **Language Modeling**:\n",
        "   - Predicting the next word in a sequence based on previous words.\n",
        "\n",
        "2. **Machine Translation**:\n",
        "   - Translating text from one language to another.\n",
        "\n",
        "3. **Text Classification**:\n",
        "   - Assigning labels to text documents, such as sentiment analysis.\n",
        "\n",
        "4. **Named Entity Recognition (NER)**:\n",
        "   - Identifying and classifying entities (e.g., names, locations) in text.\n",
        "\n",
        "5. **Speech Recognition**:\n",
        "   - Converting spoken language into text.\n",
        "\n",
        "In summary, the GRU model is a streamlined and efficient variant of RNNs that simplifies the handling of sequential data while effectively capturing long-term dependencies. Its structure‚Äîcomprising update and reset gates‚Äîallows it to dynamically control the flow of information and adapt to various NLP tasks with reduced computational complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4BFTHiKevia"
      },
      "source": [
        "### 1. **Reset Gate**\n",
        "\n",
        "**Overview:**\n",
        "- The reset gate in a GRU determines how much of the previous hidden state should be forgotten. It is crucial for resetting the memory and enabling the network to focus on new information.\n",
        "\n",
        "**Function:**\n",
        "- The reset gate controls how much of the past information is discarded. It helps in recalibrating the hidden state based on the current input and previous hidden state, especially when new information needs to be incorporated.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The reset gate $ \\ r_t \\ $ is calculated using the sigmoid activation function:\n",
        "  $$ \\\n",
        "  r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
        "  $$ \\\n",
        "  Where:\n",
        "  - $ \\ \\sigma \\ $ denotes the sigmoid function, which outputs values between 0 and 1.\n",
        "  - $ \\ W_r \\ $ is the weight matrix for the reset gate.\n",
        "  - $ \\ h_{t-1} \\ $ is the hidden state from the previous time step.\n",
        "  - $ \\ x_t \\ $ is the input at the current time step.\n",
        "  - $ \\ b_r \\ $ is the bias term for the reset gate.\n",
        "\n",
        "**Behavior:**\n",
        "- When $ \\ r_t \\ $ is close to 0, it indicates that the influence of the previous hidden state $ \\ h_{t-1} \\ $ is minimal, meaning that the network should rely more on the current input.\n",
        "- When $ \\ r_t \\ $ is close to 1, the previous hidden state retains its significance in the computation of the new candidate activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goEwaoPCevif"
      },
      "source": [
        "### 2. **Candidate Activation**\n",
        "\n",
        "**Overview:**\n",
        "- The candidate activation  $ \\tilde{h}_t \\ $ represents the new information that could be added to the hidden state. It is the result of applying a transformation to the previous hidden state, influenced by the reset gate.\n",
        "\n",
        "**Function:**\n",
        "- The candidate activation is a potential update to the hidden state. It‚Äôs computed by combining the previous hidden state (after applying the reset gate) with the current input, and it reflects the new information that might be incorporated into the hidden state.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The candidate activation $ \\tilde{h}_t \\ $ is calculated as:\n",
        "  $$\\\n",
        "  \\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)\n",
        "  $$\\\n",
        "  Where:\n",
        "  - $ \\ \\tanh \\ $ is the hyperbolic tangent function, which squashes the values to be between -1 and 1.\n",
        "  - $ \\ W_h \\ $ is the weight matrix for the candidate activation.\n",
        "  - $ \\ \\odot \\ $ denotes element-wise multiplication.\n",
        "  - $ \\ b_h \\ $ is the bias term for the candidate activation.\n",
        "\n",
        "**Behavior:**\n",
        "- The candidate activation  $ \\tilde{h}_t \\ $ is influenced by the previous hidden state adjusted by the reset gate $ \\ r_t \\ $. This allows the GRU to incorporate new information while potentially discarding less relevant past information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rEs5i7Cevif"
      },
      "source": [
        "### 3. **Update Gate**\n",
        "\n",
        "**Overview:**\n",
        "- The update gate $ \\ z_t \\ $ controls how much of the previous hidden state should be retained and how much new information should be incorporated into the hidden state. It effectively combines the roles of the forget and input gates found in LSTMs.\n",
        "\n",
        "**Function:**\n",
        "- The update gate decides the extent to which the old hidden state $ \\ h_{t-1} \\ $ is preserved and how much the new candidate activation  $ \\tilde{h}_t \\ $ should contribute to the new hidden state.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The update gate $ \\ z_t \\ $ is calculated using the sigmoid activation function:\n",
        "  $$ \\\n",
        "  z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
        "  $$ \\\n",
        "  Where:\n",
        "  - $ \\ \\sigma \\ $ denotes the sigmoid function.\n",
        "  - $ \\ W_z \\ $ is the weight matrix for the update gate.\n",
        "  - $ \\ h_{t-1} \\ $ is the hidden state from the previous time step.\n",
        "  - $ \\ x_t \\ $ is the input at the current time step.\n",
        "  - $ \\ b_z \\ $ is the bias term for the update gate.\n",
        "\n",
        "**Behavior:**\n",
        "- When $ \\ z_t \\ $ is close to 1, the new hidden state $ \\ h_t \\ $ is more influenced by the previous hidden state $ \\ h_{t-1} \\ $, implying less change.\n",
        "- When $ \\ z_t \\ $ is close to 0, the new hidden state $ \\ h_t \\ $ is more influenced by the candidate activation  $ \\tilde{h}_t \\ $, implying more change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T-3Y76revif"
      },
      "source": [
        "### 4. **Hidden State**\n",
        "\n",
        "**Overview:**\n",
        "- The hidden state $ \\ h_t \\ $ represents the output of the GRU unit and is the updated state that will be passed to the next time step. It combines the previous hidden state with the new candidate activation based on the update gate.\n",
        "\n",
        "**Function:**\n",
        "- The hidden state $ \\ h_t \\ $ captures the context of the input sequence up to the current time step. It integrates past information (controlled by the update gate) and new information (represented by the candidate activation).\n",
        "\n",
        "**Mathematical Representation:**\n",
        "- The new hidden state $ \\ h_t \\ $ is computed as:\n",
        "  $$ \\\n",
        "  h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t\n",
        "  \\ $$\n",
        "  Where:\n",
        "  - $ \\ z_t \\ $ is the update gate vector.\n",
        "  - $ \\ h_{t-1} \\ $ is the previous hidden state.\n",
        "  - $ \\tilde{h}_t \\ $ is the candidate activation.\n",
        "\n",
        "**Behavior:**\n",
        "- The final hidden state $ \\ h_t \\ $ is a weighted combination of the previous hidden state $ \\ h_{t-1} \\ $ and the candidate activation $ \\tilde{h}_t \\ $, where the weights are determined by the update gate $ \\ z_t \\ $. This allows the GRU to smoothly transition between old and new information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVfTw8RSevig"
      },
      "source": [
        "### Summary\n",
        "\n",
        "- **Reset Gate $ \\ r_t \\ $**: Controls how much of the previous hidden state should be forgotten. It helps the GRU decide which past information is less relevant for the current time step.\n",
        "\n",
        "- **Candidate Activation $ \\tilde{h}_t \\ $**: Represents the new potential update to the hidden state, influenced by the reset gate. It combines the previous hidden state with the current input to propose new information.\n",
        "\n",
        "- **Update Gate $ \\ z_t \\ $**: Determines how much of the previous hidden state should be retained and how much of the new candidate activation should be incorporated into the current hidden state. It merges the roles of the forget and input gates.\n",
        "\n",
        "- **Hidden State $ \\ h_t \\ $**: The final output of the GRU unit, combining past information (previous hidden state) and new information (candidate activation) based on the update gate. It represents the updated memory of the GRU.\n",
        "\n",
        "By managing the flow of information through these gates, GRUs can effectively capture and utilize dependencies in sequential data, making them suitable for various tasks in natural language processing and other domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOaT2YKievig"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/gru2.png\" width=\"50%\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/gru3.png\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8gy9uxEwevig",
        "outputId": "363104a7-6e54-4e11-dead-462d53f27b91"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Could not interpret activation function identifier: subtract",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-85921eda257e>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Final Dense layer for output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-85921eda257e>\u001b[0m in \u001b[0;36mgru_cell\u001b[0;34m(input, prev_hidden_state, units)\u001b[0m\n\u001b[1;32m     33\u001b[0m     hidden_state = Add()([\n\u001b[1;32m     34\u001b[0m         \u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# h_t-1 * z_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subtract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mupdate_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_hidden_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1 - z_t) * h~\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     ])\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activation, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_masking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/activations/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;34mf\"Could not interpret activation function identifier: {identifier}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret activation function identifier: subtract"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import concatenate, Dense, Multiply, Activation, Add\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
        "embedding_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# GRU custom cell implementation\n",
        "def gru_cell(input, prev_hidden_state, units):\n",
        "    # Update gate\n",
        "    update_gate = Dense(units, activation='sigmoid')(concatenate([input, prev_hidden_state]))\n",
        "\n",
        "    # Reset gate\n",
        "    reset_gate = Dense(units, activation='sigmoid')(concatenate([input, prev_hidden_state]))\n",
        "\n",
        "    # Candidate hidden state (h~)\n",
        "    candidate_hidden_state = Dense(units, activation='tanh')(concatenate([input, Multiply()([reset_gate, prev_hidden_state])]))\n",
        "\n",
        "    # Final hidden state\n",
        "    hidden_state = Add()([\n",
        "        Multiply()([update_gate, prev_hidden_state]),  # h_t-1 * z_t\n",
        "        Multiply()([Activation('subtract')(1.0 - update_gate), candidate_hidden_state])  # (1 - z_t) * h~\n",
        "    ])\n",
        "\n",
        "    return hidden_state\n",
        "\n",
        "# Build the GRU model\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding = Embedding(max_features, embedding_size)(inputs)\n",
        "\n",
        "# Initialize states for the first time step\n",
        "hidden_state = tf.zeros((batch_size, 128))\n",
        "\n",
        "# Process each time step through the GRU cell\n",
        "for t in range(maxlen):\n",
        "    current_input = embedding[:, t, :]\n",
        "    hidden_state = gru_cell(current_input, hidden_state, 128)\n",
        "\n",
        "# Final Dense layer for output\n",
        "output = Dense(1, activation='sigmoid')(hidden_state)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axzKydRQevig",
        "outputId": "ba6e1da4-f5db-4e01-b86a-32ae1f3a5d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 693ms/step - accuracy: 0.6822 - loss: 0.5727 - val_accuracy: 0.8212 - val_loss: 0.4195\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 683ms/step - accuracy: 0.8630 - loss: 0.3256 - val_accuracy: 0.8634 - val_loss: 0.3327\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 681ms/step - accuracy: 0.9159 - loss: 0.2221 - val_accuracy: 0.8740 - val_loss: 0.3066\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 692ms/step - accuracy: 0.9635 - loss: 0.1081 - val_accuracy: 0.8804 - val_loss: 0.3248\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 688ms/step - accuracy: 0.9830 - loss: 0.0574 - val_accuracy: 0.8620 - val_loss: 0.4078\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 691ms/step - accuracy: 0.9920 - loss: 0.0279 - val_accuracy: 0.8726 - val_loss: 0.5058\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 685ms/step - accuracy: 0.9941 - loss: 0.0176 - val_accuracy: 0.8632 - val_loss: 0.5500\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 690ms/step - accuracy: 0.9963 - loss: 0.0134 - val_accuracy: 0.8698 - val_loss: 0.5730\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 692ms/step - accuracy: 0.9977 - loss: 0.0125 - val_accuracy: 0.8702 - val_loss: 0.7137\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 683ms/step - accuracy: 0.9993 - loss: 0.0036 - val_accuracy: 0.8724 - val_loss: 0.7224\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 134ms/step - accuracy: 0.8582 - loss: 0.8050\n",
            "Test Accuracy: 0.8606\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "# Load the data (IMDb movie reviews)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure all input data has the same length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(GRU(128))  # GRU layer with 128 units\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1C-5Oduevih"
      },
      "source": [
        "##  GRU Applications in NLP\n",
        "\n",
        "GRUs are often used in the same contexts as LSTMs, such as:\n",
        "\n",
        "- **Machine Translation**: Translating text from one language to another.\n",
        "\n",
        "- **Text Generation**: Generating coherent and contextually relevant text.\n",
        "\n",
        "- **Speech Recognition**: Converting spoken language into text.\n",
        "\n",
        "They offer similar performance with fewer parameters, making them faster to train and easier to implement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x01ib1N1evih"
      },
      "source": [
        "# Transformers: The Modern Approach\n",
        "\n",
        "**Introduction to Transformers**\n",
        "\n",
        "- **Revolutionizing NLP**: Transformers have become the standard architecture for many NLP tasks. Unlike RNNs and their variants, transformers do not process data sequentially. Instead, they rely on a mechanism called self-attention to process all elements of the input sequence simultaneously, allowing them to capture long-range dependencies more effectively.\n",
        "\n",
        "- **Self-Attention Mechanism**:\n",
        "\n",
        "- **Attention Scores**: Each word in a sequence is assigned a score based on its relevance to other words in the sequence. This is achieved using queries, keys, and values:\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"./files/OIP.jfif\" width=\"50%\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ8Sx5RQevii"
      },
      "source": [
        "# Transformers: The Modern Approach\n",
        "\n",
        "-  **Introduction to Transformers**\n",
        "\n",
        "    - **Revolutionizing NLP**: Transformers have become the standard architecture for many NLP tasks. Unlike RNNs and their variants, transformers do not process data sequentially. Instead, they rely on a mechanism called self-attention to process all elements of the input sequence simultaneously, allowing them to capture long-range dependencies more effectively.\n",
        "\n",
        "    - **Self-Attention Mechanism**:\n",
        "\n",
        "     - **Attention Scores**: Each word in a sequence is assigned a score based on its relevance to other words in the sequence. This is achieved using queries (ùëÑ), keys (ùêæ), and values (ùëâ), which are matrices derived from the input, and ùëëùëò is the dimensionality of the keys.\n",
        "\n",
        "     - **Multi-Head Attention**: The self-attention mechanism is applied multiple times in parallel, with different weight matrices, to capture different aspects of the input. The outputs are concatenated and linearly transformed.\n",
        "\n",
        "     - **Positional Encoding**: Since transformers do not inherently process data sequentially, positional encoding is added to the input embeddings to give the model information about the position of each word in the sequence.\n",
        "\n",
        "-  **Transformer Architecture**\n",
        "\n",
        "    - **Encoder-Decoder Structure**: The original transformer model consists of an encoder (which processes the input sequence) and a decoder (which generates the output sequence).\n",
        "\n",
        "    - **Encoder**: A stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feedforward network.\n",
        "    \n",
        "    - **Decoder**: Similar to the encoder but with an additional multi-head attention mechanism that attends to the encoder's output.\n",
        "\n",
        "- **Applications**:\n",
        "\n",
        "  - **BERT (Bidirectional Encoder Representations from Transformers)**: A transformer-based model pre-trained on a large corpus of text in a bidirectional manner, making it highly effective for a variety of NLP tasks such as question answering and named entity recognition.\n",
        "\n",
        "  - **GPT (Generative Pretrained Transformer)**: A transformer model designed for text generation. It is trained in an autoregressive manner, predicting the next word in a sequence.\n",
        "\n",
        "  - **T5 (Text-To-Text Transfer Transformer)**: A model that treats every NLP problem as a text-to-text problem, enabling it to perform a wide range of tasks, from translation to summarization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3u3YcWjevii"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl574lEdevii"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnqRiSu7evii"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z18CWL0hevii"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkya8jDLevij"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIJ6Axbeevij"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WoZvJAUevij"
      },
      "source": [
        "Getting Started with NLTK\n",
        "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. Let's start with a simple example using NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtgJhvMLevij"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"Natural language processing is a subfield of artificial intelligence.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Part-of-Speech Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWa7v3yeevik"
      },
      "source": [
        "This example demonstrates basic tokenization and part-of-speech tagging using NLTK. In the upcoming sections, we'll dive deeper into these concepts and explore more advanced NLP techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VnP9pZQevik"
      },
      "source": [
        "# Text Preprocessing in NLP\n",
        "\n",
        "####  Text preprocessing is a crucial step in NLP that involves cleaning and transforming raw text data into a format suitable for analysis. This process helps to reduce noise in the text and improve the performance of NLP models.\n",
        "\n",
        " **Common Preprocessing Steps**\n",
        "\n",
        "\n",
        "- **1.Lowercasing**: Converting all text to lowercase to ensure consistency.\n",
        "\n",
        "- **2.Removing punctuation**: Eliminating punctuation marks that may not contribute to the meaning.\n",
        "\n",
        "- **3.Removing numbers**: Removing numerical digits if they're not relevant to the analysis.\n",
        "\n",
        "- **4.Removing whitespace**: Stripping extra spaces, tabs, and newlines.\n",
        "\n",
        "- **5.Removing stop words**: Eliminating common words that don't carry much meaning (e.g., \"the\", \"is\", \"at\").\n",
        "\n",
        "- **6.Stemming**: Reducing words to their root form (e.g., \"running\" to \"run\").\n",
        "\n",
        "- **7.Lemmatization**: Similar to stemming, but ensures the root word is a valid word (e.g., \"better\" to \"good\").\n",
        "\n",
        "- **8.Handling contractions**: Expanding contractions to their full form (e.g., \"don't\" to \"do not\").\n",
        "\n",
        "- **9.Removing HTML tags**: Cleaning text scraped from websites.\n",
        "\n",
        "- **10.Handling emojis and special characters**: Deciding whether to remove, replace, or keep these elements.\n",
        "\n",
        "\n",
        "\n",
        "Preprocessing with NLTK and spaCy\n",
        "We'll demonstrate text preprocessing using both NLTK and spaCy, two popular NLP libraries in Python.\n",
        "NLTK Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwZyVJ9Yevil"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text_nltk(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Example usage\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
        "preprocessed_text = preprocess_text_nltk(text)\n",
        "print(preprocessed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX9i-ekEevil"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenize and lemmatize\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_digit]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Example usage\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs! They've been doing this for 123 days.\"\n",
        "preprocessed_text = preprocess_text_spacy(text)\n",
        "print(preprocessed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9mQpXCkevin"
      },
      "source": [
        "In the next sections, we'll explore how to use these preprocessed texts for various NLP tasks such as feature extraction and text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDucB8Wcevio"
      },
      "source": [
        "# Feature Extraction in NLP\n",
        "\n",
        "Feature extraction is the process of transforming raw text data into numerical features that can be used by machine learning algorithms. This step is crucial in NLP as it bridges the gap between human-readable text and machine-understandable input.\n",
        "\n",
        "**Common Feature Extraction Techniques**\n",
        "\n",
        "\n",
        "\n",
        "- **1.Bag of Words (BoW)**: Represents text as a multiset of words, disregarding grammar and word order.\n",
        "\n",
        "- **2.Term Frequency-Inverse Document Frequency (TF-IDF)**: Reflects the importance of a word in a document within a collection.\n",
        "\n",
        "- **3.Word Embeddings**: Dense vector representations of words that capture semantic meanings.\n",
        "\n",
        "- **4.N-grams**: Contiguous sequences of n items from a given text.\n",
        "\n",
        "- **5.Part-of-Speech (POS) Features**: Grammatical features based on the role of words in sentences.\n",
        "\n",
        "- **6.Named Entity Recognition (NER) Features**: Features based on identified named entities in the text.\n",
        "\n",
        "- **7.Syntactic Features**: Based on the syntactic structure of sentences (e.g., dependency parsing).\n",
        "\n",
        "\n",
        "\n",
        "Implementing Feature Extraction\n",
        "We'll demonstrate how to implement Bag of Words, TF-IDF, and Word Embeddings using popular Python libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiBV-wkievio"
      },
      "source": [
        "Bag of Words (BoW) with scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_0YOilCevio"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample texts\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog sleeps all day.\",\n",
        "    \"The quick brown fox is quick.\"\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the texts\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the BoW representation\n",
        "print(\"Bag of Words representation:\")\n",
        "print(X.toarray())\n",
        "print(\"Feature names:\", feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXqPe82evio"
      },
      "source": [
        "TF-IDF with scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCFJTstGevio"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample texts (same as before)\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog sleeps all day.\",\n",
        "    \"The quick brown fox is quick.\"\n",
        "]\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the texts\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF representation\n",
        "print(\"TF-IDF representation:\")\n",
        "print(X.toarray())\n",
        "print(\"Feature names:\", feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHMr_FxMevip"
      },
      "source": [
        "Word Embeddings with Gensim (Word2Vec):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omREfaSUevip"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample texts (same as before)\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog sleeps all day.\",\n",
        "    \"The quick brown fox is quick.\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenized_corpus = [word_tokenize(text.lower()) for text in corpus]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get the vector for a specific word\n",
        "print(\"Vector for 'fox':\", model.wv['fox'])\n",
        "\n",
        "# Find similar words\n",
        "print(\"Words similar to 'quick':\", model.wv.most_similar('quick'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCvwFyoPevip"
      },
      "source": [
        "These examples demonstrate how to extract features from text data using different techniques. In the next sections, we'll explore how to use these features for various NLP tasks such as text classification and clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lenWgSzUevip"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
